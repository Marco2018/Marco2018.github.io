### PCA 

PCA 主成分分析 principal Components Analysis

PCA的目的是在于寻找到数据中的主成分。我们认为数据分布的更为分散，也意味着数在这个方向上的方差更大。在信号处理领域，我们认为数据具有更大的方差而噪声具有较小的方差。

对于给定的一组数据点
$$
\{v_1,v_2,...,v_n\}
$$
首先进行中心化，即
$$
\{x_1,x_2,...,x_n\}=\{v_1-\mu,v_2-\mu,...,v_n-\mu\}
$$
其中
$$
\mu=\frac{1}{n}\sum_{i=1}^nv_i
$$
x经过投影之后的数据的方差就是协方差矩阵的特征值。我们要找到最大的方差也就是协方差矩阵最大的特征值，最佳投影方向就是最大特征值所对应的特征向量。



##### 样本协方差矩阵

假设有m个样本，n个随机变量，即样本是n维的。第k维的特征与第l维的特征的协方差矩阵是：
$$
\sigma(x_k,x_l)=\frac{1}{m-1}\sum_{i=1}^m(x_{ki}-\overline{x_k})(x_{li}-\overline{x_l})
$$
协方差矩阵为：
$$
\left[
 \begin{matrix}
   \sigma(x_1,x_1) & . & . & . & \sigma(x_1,x_n)\\
   . &   &  &  &  .\\
   . &   &  &  &  .  \\
   . &   &  &  &  . \\
   \sigma(x_n,x_1) & . & . & . & \sigma(x_n,x_n)
  \end{matrix}
  \right] \tag{3}
$$


因此PCA算法的求解流程如下：
1.对样本数据进行中心化处理

2.求样本协方差矩阵

3.对协方差矩阵进行特征值分解，将特征值从大到小排列

4.去特征值中前k个特征向量$\omega_1,\omega_2,...,\omega_k$

5.做下列映射将n维样本$x_i$映射到k维
$$
x'_i=
\left[
 \begin{matrix}
   \omega_1^Tx_i\\
   \omega_2^Tx_i \\
   . \\
   . \\
   \omega_k^Tx_i
  \end{matrix}
  \right] \tag{3}
$$
新的样本$x_i'$的第d维即为样本在第d个主成分方向$\omega_d$方向上的投影。降维后信息占比为
$$
\eta=\sqrt{\frac{\sum_{i=1}^k\lambda_i^2}{\sum_{i=1}^n\lambda_i^2}}
$$
